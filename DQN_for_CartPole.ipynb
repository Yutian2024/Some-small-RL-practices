{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "eHJoXK500XOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fj6kZwa0ZGZ",
        "outputId": "ed1dfce3-2a2d-408a-b6ce-afed216c3824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.13).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "uxynp6dj0bBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVi_PolVyqJJ",
        "outputId": "21899bd7-e530-411b-bf0a-73d353dc30ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 12.0, Epsilon: 0.99\n",
            "Episode: 2, Total Reward: 12.0, Epsilon: 0.99\n",
            "Episode: 3, Total Reward: 32.0, Epsilon: 0.99\n",
            "Episode: 4, Total Reward: 42.0, Epsilon: 0.98\n",
            "Episode: 5, Total Reward: 36.0, Epsilon: 0.98\n",
            "Episode: 6, Total Reward: 20.0, Epsilon: 0.97\n",
            "Episode: 7, Total Reward: 15.0, Epsilon: 0.97\n",
            "Episode: 8, Total Reward: 20.0, Epsilon: 0.96\n",
            "Episode: 9, Total Reward: 15.0, Epsilon: 0.96\n",
            "Episode: 10, Total Reward: 11.0, Epsilon: 0.95\n",
            "Episode: 11, Total Reward: 29.0, Epsilon: 0.95\n",
            "Episode: 12, Total Reward: 19.0, Epsilon: 0.94\n",
            "Episode: 13, Total Reward: 26.0, Epsilon: 0.94\n",
            "Episode: 14, Total Reward: 15.0, Epsilon: 0.93\n",
            "Episode: 15, Total Reward: 13.0, Epsilon: 0.93\n",
            "Episode: 16, Total Reward: 56.0, Epsilon: 0.92\n",
            "Episode: 17, Total Reward: 24.0, Epsilon: 0.92\n",
            "Episode: 18, Total Reward: 12.0, Epsilon: 0.91\n",
            "Episode: 19, Total Reward: 46.0, Epsilon: 0.91\n",
            "Episode: 20, Total Reward: 56.0, Epsilon: 0.90\n",
            "Episode: 21, Total Reward: 35.0, Epsilon: 0.90\n",
            "Episode: 22, Total Reward: 14.0, Epsilon: 0.90\n",
            "Episode: 23, Total Reward: 9.0, Epsilon: 0.89\n",
            "Episode: 24, Total Reward: 11.0, Epsilon: 0.89\n",
            "Episode: 25, Total Reward: 20.0, Epsilon: 0.88\n",
            "Episode: 26, Total Reward: 17.0, Epsilon: 0.88\n",
            "Episode: 27, Total Reward: 49.0, Epsilon: 0.87\n",
            "Episode: 28, Total Reward: 13.0, Epsilon: 0.87\n",
            "Episode: 29, Total Reward: 20.0, Epsilon: 0.86\n",
            "Episode: 30, Total Reward: 15.0, Epsilon: 0.86\n",
            "Episode: 31, Total Reward: 39.0, Epsilon: 0.86\n",
            "Episode: 32, Total Reward: 25.0, Epsilon: 0.85\n",
            "Episode: 33, Total Reward: 17.0, Epsilon: 0.85\n",
            "Episode: 34, Total Reward: 28.0, Epsilon: 0.84\n",
            "Episode: 35, Total Reward: 16.0, Epsilon: 0.84\n",
            "Episode: 36, Total Reward: 16.0, Epsilon: 0.83\n",
            "Episode: 37, Total Reward: 22.0, Epsilon: 0.83\n",
            "Episode: 38, Total Reward: 29.0, Epsilon: 0.83\n",
            "Episode: 39, Total Reward: 18.0, Epsilon: 0.82\n",
            "Episode: 40, Total Reward: 18.0, Epsilon: 0.82\n",
            "Episode: 41, Total Reward: 12.0, Epsilon: 0.81\n",
            "Episode: 42, Total Reward: 14.0, Epsilon: 0.81\n",
            "Episode: 43, Total Reward: 28.0, Epsilon: 0.81\n",
            "Episode: 44, Total Reward: 32.0, Epsilon: 0.80\n",
            "Episode: 45, Total Reward: 14.0, Epsilon: 0.80\n",
            "Episode: 46, Total Reward: 41.0, Epsilon: 0.79\n",
            "Episode: 47, Total Reward: 45.0, Epsilon: 0.79\n",
            "Episode: 48, Total Reward: 14.0, Epsilon: 0.79\n",
            "Episode: 49, Total Reward: 24.0, Epsilon: 0.78\n",
            "Episode: 50, Total Reward: 20.0, Epsilon: 0.78\n",
            "Episode: 51, Total Reward: 16.0, Epsilon: 0.77\n",
            "Episode: 52, Total Reward: 31.0, Epsilon: 0.77\n",
            "Episode: 53, Total Reward: 29.0, Epsilon: 0.77\n",
            "Episode: 54, Total Reward: 19.0, Epsilon: 0.76\n",
            "Episode: 55, Total Reward: 15.0, Epsilon: 0.76\n",
            "Episode: 56, Total Reward: 11.0, Epsilon: 0.76\n",
            "Episode: 57, Total Reward: 34.0, Epsilon: 0.75\n",
            "Episode: 58, Total Reward: 25.0, Epsilon: 0.75\n",
            "Episode: 59, Total Reward: 23.0, Epsilon: 0.74\n",
            "Episode: 60, Total Reward: 32.0, Epsilon: 0.74\n",
            "Episode: 61, Total Reward: 18.0, Epsilon: 0.74\n",
            "Episode: 62, Total Reward: 13.0, Epsilon: 0.73\n",
            "Episode: 63, Total Reward: 39.0, Epsilon: 0.73\n",
            "Episode: 64, Total Reward: 14.0, Epsilon: 0.73\n",
            "Episode: 65, Total Reward: 11.0, Epsilon: 0.72\n",
            "Episode: 66, Total Reward: 52.0, Epsilon: 0.72\n",
            "Episode: 67, Total Reward: 17.0, Epsilon: 0.71\n",
            "Episode: 68, Total Reward: 13.0, Epsilon: 0.71\n",
            "Episode: 69, Total Reward: 11.0, Epsilon: 0.71\n",
            "Episode: 70, Total Reward: 13.0, Epsilon: 0.70\n",
            "Episode: 71, Total Reward: 31.0, Epsilon: 0.70\n",
            "Episode: 72, Total Reward: 21.0, Epsilon: 0.70\n",
            "Episode: 73, Total Reward: 19.0, Epsilon: 0.69\n",
            "Episode: 74, Total Reward: 10.0, Epsilon: 0.69\n",
            "Episode: 75, Total Reward: 14.0, Epsilon: 0.69\n",
            "Episode: 76, Total Reward: 44.0, Epsilon: 0.68\n",
            "Episode: 77, Total Reward: 13.0, Epsilon: 0.68\n",
            "Episode: 78, Total Reward: 9.0, Epsilon: 0.68\n",
            "Episode: 79, Total Reward: 15.0, Epsilon: 0.67\n",
            "Episode: 80, Total Reward: 9.0, Epsilon: 0.67\n",
            "Episode: 81, Total Reward: 10.0, Epsilon: 0.67\n",
            "Episode: 82, Total Reward: 10.0, Epsilon: 0.66\n",
            "Episode: 83, Total Reward: 14.0, Epsilon: 0.66\n",
            "Episode: 84, Total Reward: 19.0, Epsilon: 0.66\n",
            "Episode: 85, Total Reward: 20.0, Epsilon: 0.65\n",
            "Episode: 86, Total Reward: 30.0, Epsilon: 0.65\n",
            "Episode: 87, Total Reward: 11.0, Epsilon: 0.65\n",
            "Episode: 88, Total Reward: 17.0, Epsilon: 0.64\n",
            "Episode: 89, Total Reward: 13.0, Epsilon: 0.64\n",
            "Episode: 90, Total Reward: 18.0, Epsilon: 0.64\n",
            "Episode: 91, Total Reward: 16.0, Epsilon: 0.63\n",
            "Episode: 92, Total Reward: 25.0, Epsilon: 0.63\n",
            "Episode: 93, Total Reward: 10.0, Epsilon: 0.63\n",
            "Episode: 94, Total Reward: 11.0, Epsilon: 0.62\n",
            "Episode: 95, Total Reward: 11.0, Epsilon: 0.62\n",
            "Episode: 96, Total Reward: 12.0, Epsilon: 0.62\n",
            "Episode: 97, Total Reward: 19.0, Epsilon: 0.61\n",
            "Episode: 98, Total Reward: 20.0, Epsilon: 0.61\n",
            "Episode: 99, Total Reward: 11.0, Epsilon: 0.61\n",
            "Episode: 100, Total Reward: 12.0, Epsilon: 0.61\n",
            "Episode: 101, Total Reward: 11.0, Epsilon: 0.60\n",
            "Episode: 102, Total Reward: 10.0, Epsilon: 0.60\n",
            "Episode: 103, Total Reward: 12.0, Epsilon: 0.60\n",
            "Episode: 104, Total Reward: 10.0, Epsilon: 0.59\n",
            "Episode: 105, Total Reward: 12.0, Epsilon: 0.59\n",
            "Episode: 106, Total Reward: 28.0, Epsilon: 0.59\n",
            "Episode: 107, Total Reward: 9.0, Epsilon: 0.58\n",
            "Episode: 108, Total Reward: 53.0, Epsilon: 0.58\n",
            "Episode: 109, Total Reward: 50.0, Epsilon: 0.58\n",
            "Episode: 110, Total Reward: 10.0, Epsilon: 0.58\n",
            "Episode: 111, Total Reward: 15.0, Epsilon: 0.57\n",
            "Episode: 112, Total Reward: 19.0, Epsilon: 0.57\n",
            "Episode: 113, Total Reward: 12.0, Epsilon: 0.57\n",
            "Episode: 114, Total Reward: 14.0, Epsilon: 0.56\n",
            "Episode: 115, Total Reward: 24.0, Epsilon: 0.56\n",
            "Episode: 116, Total Reward: 14.0, Epsilon: 0.56\n",
            "Episode: 117, Total Reward: 59.0, Epsilon: 0.56\n",
            "Episode: 118, Total Reward: 53.0, Epsilon: 0.55\n",
            "Episode: 119, Total Reward: 39.0, Epsilon: 0.55\n",
            "Episode: 120, Total Reward: 13.0, Epsilon: 0.55\n",
            "Episode: 121, Total Reward: 13.0, Epsilon: 0.55\n",
            "Episode: 122, Total Reward: 42.0, Epsilon: 0.54\n",
            "Episode: 123, Total Reward: 25.0, Epsilon: 0.54\n",
            "Episode: 124, Total Reward: 70.0, Epsilon: 0.54\n",
            "Episode: 125, Total Reward: 87.0, Epsilon: 0.53\n",
            "Episode: 126, Total Reward: 103.0, Epsilon: 0.53\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from collections import deque\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "np.bool8 = np.bool_\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "# hyperparameters\n",
        "alpha, gamma = 0.001, 0.99\n",
        "epsilon, epsilon_min = 1.0, 0.1\n",
        "epsilon_decay = 0.995\n",
        "batch_size = 32\n",
        "memory_size = 10000\n",
        "episodes = 500\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "stateSpaceSize = env.observation_space.shape[0]\n",
        "actionSpaceSize = env.action_space.n\n",
        "\n",
        "# 3-layer artificial neural network\n",
        "def NN():\n",
        "  inputs = keras.Input(shape=(stateSpaceSize,), name=\"states\")\n",
        "  x1 = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
        "  x2 = keras.layers.Dense(32, activation=\"relu\")(x1)\n",
        "  outputs = keras.layers.Dense(actionSpaceSize, activation=\"linear\")(x2)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=alpha))\n",
        "  return model\n",
        "\n",
        "# buffer to save experiences\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "model = NN()\n",
        "replay_buffer = ReplayBuffer(memory_size)\n",
        "\n",
        "def train_model():\n",
        "  global epsilon\n",
        "  for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, stateSpaceSize])\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      # epsilon-greedy\n",
        "      if np.random.rand() > epsilon:\n",
        "        Q = model.predict(state, verbose=0)\n",
        "        action = np.argmax(Q[0])\n",
        "      else:\n",
        "        action = np.random.choice(actionSpaceSize)\n",
        "\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_state = np.reshape(next_state, [1, stateSpaceSize])\n",
        "      total_reward += reward\n",
        "\n",
        "      replay_buffer.add(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "\n",
        "      if len(replay_buffer) >= batch_size:\n",
        "        batch = replay_buffer.sample(batch_size)\n",
        "        states = np.array([transition[0] for transition in batch])\n",
        "        actions = np.array([transition[1] for transition in batch])\n",
        "        rewards = np.array([transition[2] for transition in batch])\n",
        "        next_states = np.array([transition[3] for transition in batch])\n",
        "        dones = np.array([transition[4] for transition in batch])\n",
        "\n",
        "        states = np.squeeze(states, axis=1)\n",
        "        next_states = np.squeeze(next_states, axis=1)\n",
        "\n",
        "        # function approximation\n",
        "        target_Q = model.predict(states, verbose=0)\n",
        "        next_Q = model.predict(next_states, verbose=0)\n",
        "        for i in range(batch_size):\n",
        "          if dones[i]:\n",
        "            target_Q[i][actions[i]] = rewards[i]\n",
        "          else:\n",
        "            target_Q[i][actions[i]] = rewards[i] + gamma * np.max(next_Q[i])\n",
        "\n",
        "        model.train_on_batch(states, target_Q)\n",
        "\n",
        "    # epsilon decay\n",
        "    if epsilon > epsilon_min:\n",
        "      epsilon *= epsilon_decay\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "    if total_reward > 100:\n",
        "      break\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, I tried to code a DQN to solve the cart pole problem.\n",
        "A 3-layer fully connected neural network is built to approximate the Q-value function.\n",
        "Stochastic gradient descent is used as the model optimizer (actually I think perhaps Adam is a better choice).\n",
        "From the training process, we can tell that at the beginning, we are mainly exploring, and thus the rewards are low.\n",
        "With the decrease of epsilon, explorations become less and the rewards are higher.\n",
        "To save time, I chose to break the look when the total reward reaches 100+.\n",
        "Better results can be achieved with more episodes."
      ],
      "metadata": {
        "id": "8DE5y61eRC21"
      }
    }
  ]
}