# -*- coding: utf-8 -*-
"""A4_Mei.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WEBFjcKp4yxohUvouXXwwFFpW_ezXYfa?usp=sharing
"""

import numpy as np
import tensorflow as tf
import keras
import gym
import matplotlib.pyplot as plt
np.bool8 = np.bool_

# hyperparameters
alpha = 0.001
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.1
epsilon_decay = 0.998
episodes = 600
n_step = 3 # n-step SARSA

env = gym.make("CartPole-v1")
state_space_size = env.observation_space.shape[0]
action_space_size = env.action_space.n

# define Q-network
def Q_network():
    model = keras.Sequential([
        keras.layers.Dense(32, activation="relu", input_shape=(state_space_size,)),
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(action_space_size, activation="linear")
    ])
    model.compile(loss="mse", optimizer=keras.optimizers.SGD(learning_rate=alpha))
    return model

model = Q_network()

def select_action(state):
    if np.random.rand() < epsilon:
        return np.random.choice(action_space_size)
    else:
        Q_values = model.predict(state, verbose=0)
        return np.argmax(Q_values[0])

def train_model():
    global epsilon
    rewards_per_episode, group_rewards, group_size = [], [], 5

    for episode in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_space_size])
        action = select_action(state)
        total_reward = 0
        done = False

        # initialize n-step buffer
        states, actions, rewards = [], [], []
        states.append(state)
        actions.append(action)

        while not done:
            next_state, reward, done, _ = env.step(action)
            next_state = np.reshape(next_state, [1, state_space_size])
            next_action = select_action(next_state)

            # store experience
            states.append(next_state)
            actions.append(next_action)
            rewards.append(reward)

            # update if enough steps or episode ends
            if len(rewards) >= n_step or done:
                # compute n-step return
                if done:
                    G = 0
                else:
                    G = model.predict(next_state, verbose=0)[0][next_action]

                # backward sum rewards
                for t in reversed(range(len(rewards))):
                    G = rewards[t] + gamma * G

                # update Q(s_t, a_t) using semi-gradient
                Q_values = model.predict(states[0], verbose=0)
                Q_values[0][actions[0]] = G  # direct assignment (semi-gradient)
                model.train_on_batch(states[0], Q_values)

                # remove oldest experience
                states.pop(0)
                actions.pop(0)
                rewards.pop(0)

            state, action = next_state, next_action
            total_reward += reward

        # epsilon decay
        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

        rewards_per_episode.append(total_reward)
        if (episode + 1) % group_size == 0:
            start_idx = max(0, episode+1-group_size)
            avg_reward = np.mean(rewards_per_episode[start_idx:episode+1])
            group_rewards.append(avg_reward)
        print(f"Episode {episode+1}: Total Reward = {total_reward}, Epsilon = {epsilon:.3f}")

        if total_reward > 200:
            break

    plt.figure(figsize=(10, 5))
    plt.plot(np.arange(len(group_rewards)) * group_size + group_size, group_rewards, 'b')
    plt.xlabel('Episode')
    plt.ylabel('Average reward (per 5 episodes)')
    plt.title('Training Curve - Grouped Average Reward')
    plt.grid(True)
    plt.show()

train_model()